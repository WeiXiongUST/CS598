# Annotate data with different types of reward models 

We consider the following representative types of reward models.
- Multi-head reward model: [ArmoRM-Llama3-8B-v0.1](https://huggingface.co/RLHFlow/ArmoRM-Llama3-8B-v0.1)
- Pairwise generative preference model: [pair-preference-model](https://huggingface.co/RLHFlow/pair-preference-model-LLaMA3-8B)
- Bradley-Terry reward model: [FsfairX-LLaMA3-RM-v0.1](https://huggingface.co/sfairXC/FsfairX-LLaMA3-RM-v0.1)


## Data format

We assume that the data is generated by the inference code of RLHF so the dataset is stored locally as a jsonl file and each sample is of the form
```python
{"prompt": ["role":'user', 'content': "the pormpt"],
"responses": ["response1", "response2", "response3", ...]}
```


The script will compute the reward for each input-output pair using different reward models, and eventually output a new dataset, where each sample contains 
```python
{"prompt": ["role":'user', 'content': "the pormpt"],
"responses": ["response1", "response2", "response3", ...]},
"rewards": [reward1, reward2, ...]}
```

## Running code

```python
accelerate launch annotate_data_bt.py --dataset_name_or_path ./data/my_local_data.jsonl --output_dir ./data/data_with_rewards.jsonl --K 4
```

